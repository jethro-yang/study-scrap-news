import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import argparse
from collections import Counter
import re
import json

def fetch_news(hours=1, search_terms=["ë‰´ìŠ¤"], media_filters=None, sort_order=0, pages=10):
    """
    ìµœì‹  ë‰´ìŠ¤ ì¤‘ ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œê°„(hours), ê²€ìƒ‰ì–´(search_terms), ì–¸ë¡ ì‚¬(media_filters)ë¡œ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
    """
    search_query = "+".join([f'"{term}"' if "*" in term else term for term in search_terms])
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    now = datetime.now()
    filtered_articles = []
    article_texts = []
    
    print(f"'{ ' '.join(search_terms) }' ê²€ìƒ‰ì–´ë¡œ ë‰´ìŠ¤ ì°¾ëŠ” ì¤‘...")
    
    for page in range(pages):
        start = page * 10
        url = f"https://www.google.com/search?q={search_query}&tbm=nws&start={start}"
        session = requests.Session()
        response = session.get(url, headers=headers)
        
        if response.status_code != 200:
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select(".WlydOe")
        
        for article in articles:
            press_tag = article.select_one("div.NUnG9d span")
            press_name = press_tag.text.strip() if press_tag else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            if media_filters and not any(m in press_name for m in media_filters):
                continue
            
            title_tag = article.select_one("div.n0jPhd")
            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
            
            link_tag = article.get("href")
            link = link_tag if link_tag else "#"
            
            time_tag = article.select_one("div.OSrXXb span")
            time_text = time_tag.text.strip() if time_tag else ""
            
            article_time = parse_relative_time(time_text)
            
            if article_time and now - article_time <= timedelta(hours=hours):
                article_data = {
                    "time": article_time.strftime('%Y-%m-%d %H:%M:%S'),
                    "press": press_name,
                    "title": title,
                    "time_text": time_text,
                    "link": link
                }
                filtered_articles.append(article_data)
                article_texts.append(title)
    
    filtered_articles.sort(key=lambda x: x["time"], reverse=(sort_order == 0))
    
    keywords = analyze_keywords(article_texts)
    save_results(filtered_articles, search_terms, hours, media_filters, sort_order, pages, keywords)

def parse_relative_time(time_text):
    """'2ì‹œê°„ ì „', '1ì¼ ì „' ê°™ì€ ìƒëŒ€ ì‹œê°„ì„ ì‹¤ì œ datetimeìœ¼ë¡œ ë³€í™˜"""
    now = datetime.now()
    
    if "ë¶„ ì „" in time_text:
        minutes = int(time_text.replace("ë¶„ ì „", "").strip())
        return now - timedelta(minutes=minutes)
    elif "ì‹œê°„ ì „" in time_text:
        hours = int(time_text.replace("ì‹œê°„ ì „", "").strip())
        return now - timedelta(hours=hours)
    elif "ì¼ ì „" in time_text:
        days = int(time_text.replace("ì¼ ì „", "").strip())
        return now - timedelta(days=days)
    
    return None

def analyze_keywords(texts):
    """ë‰´ìŠ¤ ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (3íšŒ ì´ìƒ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨, ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)"""
    combined_text = ' '.join(texts)  # ì „ì²´ ë‰´ìŠ¤ ì œëª©ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    words = re.findall(r'\b\w{2,}\b', combined_text)  # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
    
    word_counts = Counter(words)
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    common_words = [f"{word}->{count}" for word, count in sorted_words if count >= 3]
    
    print("\nğŸ“Œ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„:")
    for entry in common_words:
        print(f"- {entry}")
    
    return common_words

def save_results(articles, search_terms, hours, media_filters, sort_order, pages, keywords):
    """ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ë©°, ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€ """
    try:
        with open("news_data.json", "r", encoding="utf-8") as file:
            existing_data = json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        existing_data = []
    
    data_entry = {
        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "search_terms": search_terms,
        "hours": hours,
        "media_filters": media_filters,
        "sort_order": sort_order,
        "pages": pages,
        "articles": articles,
        "keywords": keywords
    }
    
    existing_data.append(data_entry)
    
    with open("news_data.json", "w", encoding="utf-8") as file:
        json.dump(existing_data, file, ensure_ascii=False, indent=4)
    
    print(f"âœ… ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ 'news_data.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë§ì¶¤í˜• ë‰´ìŠ¤ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("-t", type=int, default=1, help="ìµœê·¼ ëª‡ ì‹œê°„ ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ì§€ ì„¤ì •")
    parser.add_argument("-s", type=int, choices=[0, 1], default=0, help="ì •ë ¬ ë°©ì‹ (0: ìµœì‹ ìˆœ, 1: ì˜¤ë˜ëœ ìˆœ)")
    parser.add_argument("-k", nargs="+", default=["ë‰´ìŠ¤"], help="ê²€ìƒ‰í•  í‚¤ì›Œë“œ")
    parser.add_argument("-p", type=int, default=10, help="ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜")
    parser.add_argument("-m", nargs="*", default=None, help="íŠ¹ì • ì–¸ë¡ ì‚¬ í•„í„°ë§")
    
    args = parser.parse_args()
    fetch_news(hours=args.t, search_terms=args.k, media_filters=args.m, sort_order=args.s, pages=args.p)
