import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import argparse
from collections import Counter
import re

def fetch_news(hours=1, search_terms=["ë‰´ìŠ¤"], media_filters=None, sort_order=0, pages=10):
    """
    ìµœì‹  ë‰´ìŠ¤ ì¤‘ ì‚¬ìš©ìžê°€ ì§€ì •í•œ ì‹œê°„(hours), ê²€ìƒ‰ì–´(search_terms), ì–¸ë¡ ì‚¬(media_filters)ë¡œ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
    :param hours: ëª‡ ì‹œê°„ ë‚´ì˜ ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜¬ì§€ ì„¤ì • (ê¸°ë³¸ê°’: 1ì‹œê°„)
    :param search_terms: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: ["ë‰´ìŠ¤"])
    :param media_filters: íŠ¹ì • ì–¸ë¡ ì‚¬ í•„í„°ë§ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: None = ì „ì²´ ì–¸ë¡ ì‚¬)
    :param sort_order: ì •ë ¬ ë°©ì‹ (0: ìµœì‹ ìˆœ, 1: ì˜¤ëž˜ëœ ìˆœ)
    :param pages: ê²€ìƒ‰í•  íŽ˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10íŽ˜ì´ì§€)
    """
    search_query = "+".join([f'"{term}"' if "*" in term else term for term in search_terms])
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}
    now = datetime.now()
    filtered_articles = []
    article_texts = []
    
    print(f"'{ ' '.join(search_terms) }' ê²€ìƒ‰ì–´ë¡œ ë‰´ìŠ¤ ì°¾ëŠ” ì¤‘...")
    
    for page in range(pages):
        start = page * 10
        url = f"https://www.google.com/search?q={search_query}&tbm=nws&start={start}"
        session = requests.Session()
        response = session.get(url, headers=headers)
        
        if response.status_code != 200:
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select(".WlydOe")
        
        for article in articles:
            press_tag = article.select_one("div.NUnG9d span")
            press_name = press_tag.text.strip() if press_tag else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            if media_filters and not any(m in press_name for m in media_filters):
                continue
            
            title_tag = article.select_one("div.n0jPhd")
            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
            
            link_tag = article.get("href")
            link = f"Link->{link_tag}" if link_tag else "#"
            
            time_tag = article.select_one("div.OSrXXb span")
            time_text = time_tag.text.strip() if time_tag else ""
            
            article_time = parse_relative_time(time_text)
            
            if article_time and now - article_time <= timedelta(hours=hours):
                formatted_content = (article_time, f"[{press_name}] {title} ({time_text})\n    {link}\n")
                filtered_articles.append(formatted_content)
                article_texts.append(title)
    
    filtered_articles.sort(key=lambda x: x[0], reverse=(sort_order == 0))
    
    if filtered_articles:
        with open("news_texts_filtered.txt", "a", encoding="utf-8") as file:
            file.write(f"\n=== ì‹¤í–‰ ì‹œê°„: {now.strftime('%Y-%m-%d %H:%M:%S')} / ìµœê·¼ {hours}ì‹œê°„ / ê²€ìƒ‰ì–´: {' '.join(search_terms)} ===\n")
            file.writelines([f"{article[1]}\n" for article in filtered_articles])
        
        print(f"âœ… ìµœê·¼ {hours}ì‹œê°„ ë‚´ '{' '.join(search_terms)}' ê´€ë ¨ {len(filtered_articles)}ê°œ ë‰´ìŠ¤ê°€ 'news_texts_filtered.txt' íŒŒì¼ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í‚¤ì›Œë“œ ë¶„ì„
    analyze_keywords(article_texts)

def parse_relative_time(time_text):
    """'2ì‹œê°„ ì „', '1ì¼ ì „' ê°™ì€ ìƒëŒ€ ì‹œê°„ì„ ì‹¤ì œ datetimeìœ¼ë¡œ ë³€í™˜"""
    now = datetime.now()
    
    if "ë¶„ ì „" in time_text:
        minutes = int(time_text.replace("ë¶„ ì „", "").strip())
        return now - timedelta(minutes=minutes)
    elif "ì‹œê°„ ì „" in time_text:
        hours = int(time_text.replace("ì‹œê°„ ì „", "").strip())
        return now - timedelta(hours=hours)
    elif "ì¼ ì „" in time_text:
        days = int(time_text.replace("ì¼ ì „", "").strip())
        return now - timedelta(days=days)
    
    return None

def analyze_keywords(texts):
    """ë‰´ìŠ¤ ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„"""
    words = []
    for text in texts:
        words.extend(re.findall(r'\b\w{2,}\b', text))  # ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ìž ì´ìƒ)
    
    word_counts = Counter(words)
    common_words = word_counts.most_common(10)  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
    
    print("\nðŸ“Œ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„:")
    for word, count in common_words:
        print(f"- {word}: {count}íšŒ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë§žì¶¤í˜• ë‰´ìŠ¤ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("-t", type=int, default=1, help="ìµœê·¼ ëª‡ ì‹œê°„ ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ì§€ ì„¤ì • (ì •ìˆ˜, ê¸°ë³¸ê°’: 1)")
    parser.add_argument("-s", type=int, choices=[0, 1], default=0, help="ì •ë ¬ ë°©ì‹ (0: ìµœì‹  ê¸°ì‚¬ ìš°ì„ , 1: ì˜¤ëž˜ëœ ê¸°ì‚¬ ìš°ì„ )")
    parser.add_argument("-k", nargs="+", default=["ë‰´ìŠ¤"], help="ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì—¬ëŸ¬ ê°œ ìž…ë ¥ ê°€ëŠ¥, ê¸°ë³¸ê°’: 'ë‰´ìŠ¤')")
    parser.add_argument("-p", type=int, default=10, help="ê²€ìƒ‰í•  íŽ˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10 íŽ˜ì´ì§€)")
    parser.add_argument("-m", nargs="*", default=None, help="íŠ¹ì • ì–¸ë¡ ì‚¬ í•„í„°ë§ (ì˜ˆ: 'ì—°í•©', 'ì¡°ì„ ', 'ì¤‘ì•™')")
    
    args = parser.parse_args()
    fetch_news(hours=args.t, search_terms=args.k, media_filters=args.m, sort_order=args.s, pages=args.p)
